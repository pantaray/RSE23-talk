---
title: "ACME"
subtitle: "Flexible Scalability for Research Software"
author:
  - Stefan </br> Fürtinger
  - Katharine Shapcott
  - Joscha Tapani Schmiedt
date: February 23 2023
date-format: long
institute:
  - Ernst Strüngmann Institute (ESI) Frankfurt
  - Ernst Strüngmann Institute (ESI) Frankfurt
  - Brain Research Institute Universität Bremen
format:
  revealjs:
    theme: [white, css/esi.scss]
    slide-number: c
    menu: true
    progress: true
    chalkboard: false
      # buttons: false
    preview-links: auto
    css: css/agenda.css
    code-line-numbers: false
    highlight-style: github
    auto-play-media: true
    embed-resources: false
    pdf-separate-fragments: true
include-after-body: css/esi_borders.html
revealjs-plugins:
  - attribution
filters:
  - reveal-auto-agenda
auto-agenda:
  bullets: none
  heading: Outline
---

```{=html}
<section class="title-slide slide level1 agenda-slide center">
<div class="agenda-heading">
<p>Outline</p>
</div>
<div class="agenda">
<div class="agenda-active">
<p>I Big Data Is Great</p>
</div>
<div class="agenda-active">
<p>II Why ACME?</p>
</div>
<div class="agenda-active">
<p>III An Offer You Can Refuse</p>
</div>
</div>
</section>
```

# I Big Data Is Great

## Big Data: A Topical Example

::: {.r-stack}
![](imgs/fastMRI_website.png){.fragment .fade-in-then-semi-out .absolute width="85%" top="1.5em" left="1em"}
![](imgs/fastMRI_knee.png){.fragment .absolute width="40%" left="1em" top="4em"}
![](imgs/fastMRI_brain.png){.fragment .absolute width="45%" right="1em" top="1.5em"}
:::

::: {.attribution}
See [fastmri.med.nyu.edu](https://fastmri.med.nyu.edu/)
:::

# II Why ACME?

## Get SLURMed {transition="fade-in" auto-animate=true}

```{.python data-id="code1" code-line-numbers="|5-6,9-11,14-15"}
# Which subject do we want to analyze?
subIdx = 0

# Take stock of data on disk
data = datasets.fetch_development_fmri(age_group="adult")
atlasCoords = datasets.fetch_coords_power_2011()

# Extract fMRI time-series averaged within spheres @ atlas coords
mask = NiftiSpheresMasker(seeds=atlasCoords)
timeseries = mask.fit_transform(data.func[subIdx],
                                confounds=data.confounds[subIdx])

# Compute functional connectivity b/w brain regions
estimator = GraphicalLassoCV()
estimator.fit(timeseries)

# Inspect results
plotting.plot_connectome(estimator.covariance_)
```

::: {.r-stack}
![](imgs/subject0.png){.fragment fragment-index=1 .fade-in-then-out .absolute top="3em" left="5em" width="55%"}
:::

## Get SLURMed {auto-animate=true}

```{.python data-id="code1" filename="connectome.py" code-line-numbers="1-16|" style="font-size: 0.8em; "}
def compute_connectome(subIdx):

      # Take stock of data on disk
      data = datasets.fetch_development_fmri(age_group="adult")
      atlasCoords = datasets.fetch_coords_power_2011()

      # Extract fMRI time-series averaged within spheres @ atlas coords
      masker = NiftiSpheresMasker(seeds=atlasCoords)
      timeseries = masker.fit_transform(data.func[subIdx],
                                        confounds=data.confounds[subIdx])

      # Compute functional connectivity b/w brain regions
      estimator = GraphicalLassoCV()
      estimator.fit(timeseries)
      return estimator.covariance_

if __name__ == "__main__":

      # Compute functional connectivity of subject and save result
      con = compute_connectome(sys.argv[1])
      np.save("con_{}".format(sys.argv[1]), con)
```

## Get SLURMed {auto-animate=true}

```{.python data-id="code1" filename="connectome.py"}
```

. . .

```{.bash data-id="code3" filename="run_connectome.sh"}
#!/bin/bash
#
# SLURM script for computing per-subject connectomes
#
#SBATCH -J connectome_batch # Common name for the job-array
#SBATCH -p myPartition      # Partition
#SBATCH -c 2                # Use two cores
#SBATCH -t 0-2:00           # Max run-time of 2 hours
#SBATCH --mem 4000          # Request 4 GB of RAM
#SBATCH -o con_%A_%a.out    # Redirect stdout/stderr to file
#SBATCH --array=1-33        # Define job-array

source /path/to/conda/etc/profile.d/conda.sh
conda activate myenv
srun python connectome.py "$SLURM_ARRAY_TASK_ID"
```

## Get SLURMed {auto-animate=true}

```{.python data-id="code1" filename="connectome.py"}
```

```{.bash data-id="code3" filename="run_connectome.sh"}
```

. . .

```{data-id="code4"}
sbatch run_connectome.sh
```

## Get SLURMed {auto-animate=true}

```{.python data-id="code1" filename="connectome.py"}
```

```{.bash data-id="code3" filename="run_connectome.sh"}
```

```{data-id="code4"}
sbatch run_connectome.sh
Submitted batch job 21607933

squeue --me
ACCOUNT JOBID PARTITION NODELIST PRIORITY TIME STATE
fuertingers 21607933_3 8GBXS esi-svhpc46 25228545 0:11 RUNNING
fuertingers 21607933_4 8GBXS esi-svhpc46 25228545 0:11 RUNNING
fuertingers 21607933_5 8GBXS esi-svhpc29 25228545 0:11 RUNNING
fuertingers 21607933_2 8GBXS esi-svhpc24 25228545 0:15 RUNNING
fuertingers 21607933_1 8GBXS esi-svhpc24 25228545 0:16 RUNNING
...
```

## Get SLURMed {transition="fade"}

<!-- ```{=html}
<iframe src="imgs/sbatch.html" width="100%" height="80%"></iframe>
``` -->

![](imgs/sbatch_schedmd.gif)

## It All Started With A gist...

![](imgs/acme_gist.png){.position .absolute height="80%" left="2em"}

## Asynchronous Computing Made ESI

![](imgs/acme_logo.png){.position .absolute left="0em" top="2em" width="22%"}

::: {.incremental style="font-size: 0.9em; " .position .absolute left="8em" right="0em" top="2.5em"}
- accelerates "simple", i.e., [embarassingly parallel]{.esi-emph}, workloads
- [wraps]{.esi-emph} sequential code and [maps]{.esi-emph} on parallel computing hardware
- DIY parallelization via [context manager]{.esi-emph} `ParallelMap` in Python
- built on top of [[dask](https://www.dask.org/)]{.esi-emph} and
  [[dask-jobqueue](https://jobqueue.dask.org/en/latest/)]{.esi-emph} to
  integrate with HPC clusters
  ![](imgs/dask.svg){.position .absolute left="-8em" top="6em" width="30%"}
  ![](imgs/dask-jobqueue.png){.position .absolute left="-8em" top="9em" width="30%"}
:::

. . .

::: {.position .absolute top="13.5em" left="3em"}
```{.bash style="font-size: 1.3em; "}
pip install esi-acme
conda install -c conda-forge esi-acme
```
:::

## An Unpractical Example

```{.python}
def f(x, y, z=3):
    return (x + y) * z
```

[Objective:]{.esi-emph} Evaluate `f` for four different values of `x` and `y = 4`

. . .

`f(`[2]{.esi-emph}`, 4, z=3) = 18`

`f(`[4]{.esi-emph}`, 4, z=3) = 24`

`f(`[6]{.esi-emph}`, 4, z=3) = 30`

`f(`[8]{.esi-emph}`, 4, z=3) = 36`

## An Unpractical Example {auto-animate="true"}

```{.python}
def f(x, y, z=3):
    return (x + y) * z
```

## An Unpractical Example {auto-animate="true"}

```{.python}
from acme import ParallelMap

def f(x, y, z=3):
    return (x + y) * z

with ParallelMap(f, [2, 4, 6, 8], 4) as pmap:
    pmap.compute()
```

## An Unpractical Example {auto-animate="true"}

![](imgs/acme_simple.webm){.position .absolute height="100%" top="0em"}


## Back To SLURM Connectomes...  {auto-animate="true"}

. . .

```{.python data-id="pmap" filename="connectome.py" style="font-size: 0.8em; "}
def compute_connectome(subIdx):

      # Take stock of data on disk
      data = datasets.fetch_development_fmri(age_group="adult")
      atlasCoords = datasets.fetch_coords_power_2011()

      # Extract fMRI time-series averaged within spheres @ atlas coords
      masker = NiftiSpheresMasker(seeds=atlasCoords)
      timeseries = masker.fit_transform(data.func[subIdx],
                                        confounds=data.confounds[subIdx])

      # Compute functional connectivity b/w brain regions
      estimator = GraphicalLassoCV()
      estimator.fit(timeseries)
      return estimator.covariance_

if __name__ == "__main__":

      # Compute functional connectivity of subject and save result
      con = compute_connectome(sys.argv[1])
      np.save("con_{}".format(sys.argv[1]), con)
```

## Back To SLURM Connectomes...  {auto-animate=true}

```{.python data-id="pmap" filename="connectome.py" style="font-size: 0.8em; "}
def compute_connectome(subIdx):

      # Take stock of data on disk
      data = datasets.fetch_development_fmri(age_group="adult")
      atlasCoords = datasets.fetch_coords_power_2011()

      # Extract fMRI time-series averaged within spheres @ atlas coords
      masker = NiftiSpheresMasker(seeds=atlasCoords)
      timeseries = masker.fit_transform(data.func[subIdx],
                                        confounds=data.confounds[subIdx])

      # Compute functional connectivity b/w brain regions
      estimator = GraphicalLassoCV()
      estimator.fit(timeseries)
      return estimator.covariance_

if __name__ == "__main__":

      # Compute functional connectivity of subject and save result
      with ParallelMap(compute_connectome, range(21)) as pmap:
          results = pmap.compute()
```

. . .

[...[or]{.esi-emph}]{.position .absolute left="10em" bottom="1em"}

## Back To SLURM Connectomes... {auto-animate="true"}

### ...in Jupyter

```{.python data-id="jupyter1" style="font-size: 0.8em; "}
[1]: from connectome import compute_connectome, subjectList
     from acme import ParallelMap
```

## Back To SLURM Connectomes... {auto-animate="true"}

### ...in Jupyter

```{.python data-id="jupyter1" style="font-size: 0.8em; "}
[1]: from connectome import compute_connectome, subjectList
     from acme import ParallelMap
     from dask_jobqueue import SLURMCluster
```

. . .

```{.python data-id="jupyter2" style="font-size: 0.8em; "}
[2]: cluster = SLURMCluster(queue="8GBXS", cores=1, memory="8GB", processes=1,
                            local_directory="/path/to/somewhere", python=sys.executable,
                            job_directives_skip=["-t", "--mem"])
     cluster.scale(10)
```

## Back To SLURM Connectomes... {auto-animate=true}

### ...in Jupyter

```{.python data-id="jupyter1" style="font-size: 0.8em; "}
[1]: from connectome import compute_connectome, subjectList
     from acme import ParallelMap
```

## Back To SLURM Connectomes... {auto-animate=true}

### ...in Jupyter

```{.python data-id="jupyter1" style="font-size: 0.8em; "}
[1]: from connectome import compute_connectome, subjectList
     from acme import ParallelMap, esi_cluster_setup
```

. . .

```{.python data-id="jupyter2" style="font-size: 0.8em; "}
[2]: myClient = esi_cluster_setup(n_workers=10, partition="8GBXS")
```

## Back To SLURM Connectomes... {auto-animate=true}

### ...in Jupyter

```{.python data-id="jupyter1" style="font-size: 0.8em; "}
[1]: from connectome import compute_connectome, subjectList
     from acme import ParallelMap, esi_cluster_setup
```

```{.python data-id="jupyter2" style="font-size: 0.8em; "}
[2]: myClient = esi_cluster_setup(n_workers=10, partition="8GBXS")
```
```{data-id="jupyter2" style="font-size: 0.4em; "}
<slurm_cluster_setup> Requested worker-count 10 exceeds `n_workers_startup`: waiting for 1 workers to come online, then proceed
<slurm_cluster_setup> SLURM workers ready: 1/1 	[elapsed time 00:05 | timeout at 01:00]
<slurm_cluster_setup> Cluster dashboard accessible at http://10.100.32.17:40043/status
```

## Back To SLURM Connectomes... {auto-animate=true}

### ...in Jupyter

```{.python data-id="jupyter1" style="font-size: 0.8em; "}
[1]: from connectome import compute_connectome, subjectList
     from acme import ParallelMap, esi_cluster_setup
```

```{.python data-id="jupyter2" style="font-size: 0.8em; "}
[2]: myClient = esi_cluster_setup(n_workers=10, partition="8GBXS")
```
```{data-id="jupyter2" style="font-size: 0.4em; "}
<slurm_cluster_setup> Requested worker-count 10 exceeds `n_workers_startup`: waiting for 1 workers to come online, then proceed
<slurm_cluster_setup> SLURM workers ready: 1/1 	[elapsed time 00:05 | timeout at 01:00]
<slurm_cluster_setup> Cluster dashboard accessible at http://10.100.32.17:40043/status
```

. . .

```{.python data-id="jupyter3" style="font-size: 0.8em; "}
[3]: with ParallelMap(compute_connectome, subjectList, result_shape=(264, 264, None))) as pmap:
         results = pmap.compute()
```

## Back To SLURM Connectomes... {auto-animate=true}

### ...in Jupyter

```{.python data-id="jupyter1" style="font-size: 0.8em; "}
[1]: from connectome import compute_connectome, subjectList
     from acme import ParallelMap, esi_cluster_setup
```

```{.python data-id="jupyter2" style="font-size: 0.8em; "}
[2]: myClient = esi_cluster_setup(n_workers=10, partition="8GBXS")
```
```{data-id="jupyter2" style="font-size: 0.4em; "}
<slurm_cluster_setup> Requested worker-count 10 exceeds `n_workers_startup`: waiting for 1 workers to come online, then proceed
<slurm_cluster_setup> SLURM workers ready: 1/1 	[elapsed time 00:05 | timeout at 01:00]
<slurm_cluster_setup> Cluster dashboard accessible at http://10.100.32.17:40043/status
```

```{.python data-id="jupyter3" style="font-size: 0.8em; "}
[3]: with ParallelMap(compute_connectome, subjectList, result_shape=(264, 264, None))) as pmap:
         results = pmap.compute()
```
```{data-id="jupyter3" style="font-size: 0.4em; " code-line-numbers="2-3"}
<ParallelMap> INFO: This is ACME v. 2022.12
<ParallelMap> INFO: Attaching to global parallel computing client <Client: 'tcp://10.100.32.17:33661' processes=10 threads=10, memory=74.50 GiB>
```

## Back To SLURM Connectomes... {auto-animate=true}

### ...in Jupyter

```{.python data-id="jupyter1" style="font-size: 0.8em; "}
[1]: from connectome import compute_connectome, subjectList
     from acme import ParallelMap, esi_cluster_setup
```

```{.python data-id="jupyter2" style="font-size: 0.8em; "}
[2]: myClient = esi_cluster_setup(n_workers=10, partition="8GBXS")
```
```{data-id="jupyter2" style="font-size: 0.4em; "}
<slurm_cluster_setup> Requested worker-count 10 exceeds `n_workers_startup`: waiting for 1 workers to come online, then proceed
<slurm_cluster_setup> SLURM workers ready: 1/1 	[elapsed time 00:05 | timeout at 01:00]
<slurm_cluster_setup> Cluster dashboard accessible at http://10.100.32.17:40043/status
```

```{.python data-id="jupyter3" style="font-size: 0.8em; "}
[3]: with ParallelMap(compute_connectome, subjectList, result_shape=(264, 264, None))) as pmap:
         results = pmap.compute()
```
```{data-id="jupyter3" style="font-size: 0.4em; " code-line-numbers="2-3"}
<ParallelMap> INFO: This is ACME v. 2022.12
<ParallelMap> INFO: Attaching to global parallel computing client <Client: 'tcp://10.100.32.17:33661' processes=10 threads=10, memory=74.50 GiB>
```

. . .

::: {data-id="box1" style="background: rgba(0,0,0,0); width: 100%; height: 1em; border: 0.1em solid red; " .absolute top="10.5em" left="0em" .fade}
:::


## Back To SLURM Connectomes... {auto-animate=true}

### ...in Jupyter

```{.python data-id="jupyter1" style="font-size: 0.8em; "}
[1]: from connectome import compute_connectome, subjectList
     from acme import ParallelMap, esi_cluster_setup
```

```{.python data-id="jupyter2" style="font-size: 0.8em; "}
[2]: myClient = esi_cluster_setup(n_workers=10, partition="8GBXS")
```
```{data-id="jupyter2" style="font-size: 0.4em; "}
<slurm_cluster_setup> Requested worker-count 10 exceeds `n_workers_startup`: waiting for 1 workers to come online, then proceed
<slurm_cluster_setup> SLURM workers ready: 1/1 	[elapsed time 00:05 | timeout at 01:00]
<slurm_cluster_setup> Cluster dashboard accessible at http://10.100.32.17:40043/status
```

```{.python data-id="jupyter3" style="font-size: 0.8em; "}
[3]: with ParallelMap(compute_connectome, subjectList, result_shape=(264, 264, None))) as pmap:
         results = pmap.compute()
```
```{data-id="jupyter3" style="font-size: 0.4em; color: red; " code-line-numbers="2-3"}
<ParallelMap> INFO: This is ACME v. 2022.12
<ParallelMap> INFO: Attaching to global parallel computing client <Client: 'tcp://10.100.32.17:33661' processes=10 threads=10, memory=74.50 GiB>
<ParallelMap> INFO: Preparing 21 parallel calls of `compute_connectome` using 10 workers
<ParallelMap> INFO: Log information available at /cs/slurm/fuertingers/fuertingers_20230207-093941
100% |██████████| 21/21 [01:46<00:00]
<ParallelMap> INFO: SUCCESS! Finished parallel computation. Results have been saved to /cs/home/fuertingers/ACME_20230207-094014-740639/compute_connectome.h5 with links to data payload located in /cs/home/fuertingers/ACME_20230207-094014-740639/compute_connectome_payload
```

::: {data-id="box1" style="background: rgba(0,0,0,0); width: 100%; height: 1em; border: 0.1em solid red; " .absolute top="10.5em" left="0em" .fade}
:::

## Back To SLURM Connectomes... {auto-animate=true}

### ...in Jupyter

```{.python data-id="jupyter1" style="font-size: 0.8em; "}
[1]: from connectome import compute_connectome, subjectList
     from acme import ParallelMap, esi_cluster_setup
```

```{.python data-id="jupyter2" style="font-size: 0.8em; "}
[2]: myClient = esi_cluster_setup(n_workers=10, partition="8GBXS")
```
```{data-id="jupyter2" style="font-size: 0.4em; "}
<slurm_cluster_setup> Requested worker-count 10 exceeds `n_workers_startup`: waiting for 1 workers to come online, then proceed
<slurm_cluster_setup> SLURM workers ready: 1/1 	[elapsed time 00:05 | timeout at 01:00]
<slurm_cluster_setup> Cluster dashboard accessible at http://10.100.32.17:40043/status
```

```{.python data-id="jupyter3" style="font-size: 0.8em; "}
[3]: with ParallelMap(compute_connectome, subjectList, result_shape=(264, 264, None))) as pmap:
         results = pmap.compute()
```
```{data-id="jupyter3" style="font-size: 0.4em; "}
<ParallelMap> INFO: This is ACME v. 2022.12
<ParallelMap> INFO: Attaching to global parallel computing client <Client: 'tcp://10.100.32.17:33661' processes=10 threads=10, memory=74.50 GiB>
<ParallelMap> INFO: Preparing 21 parallel calls of `compute_connectome` using 10 workers
<ParallelMap> INFO: Log information available at /cs/slurm/fuertingers/fuertingers_20230207-093941
100% |██████████| 21/21 [01:46<00:00]
<ParallelMap> INFO: SUCCESS! Finished parallel computation. Results have been saved to /cs/home/fuertingers/ACME_20230207-094014-740639/compute_connectome.h5 with links to data payload located in /cs/home/fuertingers/ACME_20230207-094014-740639/compute_connectome_payload
```

::: {data-id="box1" style="background: rgba(0,0,0,0); width: 100%; height: 1.5em; border: 0.1em solid red; " .absolute top="12.9em" left="0em" .fade}
:::

. . .

[...what?]{.position .absolute left="10em" bottom="1em"}

## Back To SLURM Connectomes...

### ...in Jupyter

```{.python data-id="jupyter4" style="font-size: 0.8em; "}
[4]: pmap.results_container
```
. . .

```{data-id="jupyter4" style="font-size: 0.4em; "}
'/cs/home/fuertingers/ACME_20230206-122528-965199/compute_connectome.h5'
```

. . .

```{.python data-id="jupyter5" style="font-size: 0.8em; "}
[5]: results
```

. . .

```{data-id="jupyter5" style="font-size: 0.4em; "}
['/cs/home/fuertingers/ACME_20230206-122528-965199/compute_connectome_payload/compute_connectome_0.h5',
 '/cs/home/fuertingers/ACME_20230206-122528-965199/compute_connectome_payload/compute_connectome_1.h5',
 '/cs/home/fuertingers/ACME_20230206-122528-965199/compute_connectome_payload/compute_connectome_2.h5',
 '/cs/home/fuertingers/ACME_20230206-122528-965199/compute_connectome_payload/compute_connectome_3.h5',
 '/cs/home/fuertingers/ACME_20230206-122528-965199/compute_connectome_payload/compute_connectome_4.h5']
```

. . .

::: {.incremental style="font-size: 0.85em; "}
- single [aggregate results file]{.esi-emph} `compute_connectome.h5`
  only points to data written by workers
- each worker saves results [independently]{.esi-emph} to files in the `*_payload` directory
- leverages [HDF5 Virtual Datasets]{.esi-emph}
:::


## Communication via Filesystem

<!-- ::: {.r-stack}
![](imgs/virtual_dataset_illustration_i.png){.fragment .fade-in-then-out .absolute top="2em" right="1em" width="95%"}
![](imgs/virtual_dataset_illustration_ii.png){.fragment .fade-in-then-out .absolute top="2em" right="1em" width="95%"}
::: -->

![](imgs/virtual_dataset_illustration_ii.png){.fragment .fade-in-then-out .absolute top="2em" right="1em" width="100%"}


# III An Offer You Can Refuse

## Why Python? {auto-animate="true"}

. . .

Home to [modern ML/AI libraries]{.esi-emph}

```{.python style="font-size: 0.8em; "}
from torch.optim import Adam
from torch.nn import Sequential
from acme import ParallelMap

def hyper_par_tuning(learning_rate, epochs=10):
  model = Sequential(...)
  optimizer = Adam(model.parameters(), lr=learning_rate, betas=(0.5, 0.999))
  ...
  for epoch in range(epochs):
    classifications = model(inputs)
    loss = criterion(classifications, labels)
    loss.backward()
    optimizer.step()
    ...
```

## Why Python? {auto-animate="true"}

Home to [modern ML/AI libraries]{.esi-emph}

```{.python style="font-size: 0.8em; "}
from torch.optim import Adam
from torch.nn import Sequential
from acme import ParallelMap

def hyper_par_tuning(learning_rate, epochs=10):
  model = Sequential(...)
  optimizer = Adam(model.parameters(), lr=learning_rate, betas=(0.5, 0.999))
  ...
  for epoch in range(epochs):
    classifications = model(inputs)
    loss = criterion(classifications, labels)
    loss.backward()
    optimizer.step()
    ...
with ParallelMap(hyper_par_tuning, np.linspace(1e-3, 1e-6, 100)) as pmap:
  pmap.compute()
```

## Why Python? {auto-animate="true"}

[Interfaces]{.esi-emph}...

. . .

...with [R]{.esi-emph}

```{.R style="font-size: 0.8em; " filename="some_code.R"}
sample_stats <- function(d_mu, d_sd) {
    samp <- rnorm(10^6, d_mu, d_sd)
    c(s_mu = mean(samp), s_sd = sd(samp))
}
```

::: {style="height: 1em; "}
:::

. . .

```{.python style="font-size: 0.8em; "}
from rpy2.robjects.packages import STAP

def sample_stats(mu, sd):
  with open("some_code.R", "r") as Rfile:
    Rcode = Rfile.read()
  some_code = STAP(Rcode, "sample_stats")
  return np.array(some_code.sample_stats(mu, sd))

with ParallelMap(some_code.sample_stats, [2, 4, 6, 8], [3, 5, 7, 9]) as pmap:
  pmap.compute()
```

## Why Python? {auto-animate="true"}

[Interfaces]{.esi-emph}...

...with [system binaries]{.esi-emph}

```{.python style="font-size: 0.8em; "}
import subprocess

def freesurf_preproc(subIdx):
  cmd = "recon-all -s sub-{subj:d} -i sub-{subj:d}_ses-BL_T1w.nii.gz -all"
  subprocess.run(cmd.format(sub=subIdx), text=True, shell=True, check=True)

with ParallelMap(freesurf_preproc, np.arange(101, 151)) as pmap:
  pmap.compute()
```

. . .

...and [FORTRAN]{.esi-emph}, [Java]{.esi-emph}, [Octave]{.esi-emph}, ...

## ACME {visibilty="uncounted"}

## ACME is [not]{style="color: red"}

::: {.incremental style="font-size: 1.0em; " .absolute .position top="3em"}
- another `sbatch` wrapper
- tuned for best performance possible
- a general purpose parallelization framework
- up for tasks requiring inter-worker communication
- built for for processing shared distributed data structures
- a great way to tackle CFD problems/PDE discretizations
:::

## ACME [is]{.esi-emph}

::: {.incremental style="font-size: 0.9em; " .absolute .position bottom="1em"}
- made for [easy-to-use parallelization]{.esi-emph} of code not written with concurrency in mind
- [application-agnostic]{.esi-emph}: has been used for math optimization,
  neural networks, fMRI and is the parallelization engine of
  ![](imgs/syncopy_logo.svg){.absolute .position top="3.7em" right="2.8em" width="15%"}
- based on existing [robust parallel computing libraries]{.esi-emph} (dask, dask-jobqueue)
- [small network footprint]{.esi-emph} by shifting concurrency to filesystem
- fully [open-source on [GitHub](https://github.com/esi-neuroscience/acme)]{.esi-emph} PRs welcome!
- open to [[include more HPC centers]{.esi-epmh}]{.esi-emph}

  ``<your-institution-here>_cluster_setup``
:::

## {visibilty="uncounted"}

::: {style="height: 0.8em; "}
:::

::: {.r-fit-text}
Thank You For Your Attention!
:::

[**GitHub**]{.absolute top="10.5em" left="0em" style="font-size: 0.9em; "}

![](imgs/me.jpeg){.absolute top="4em" left="3em" width="16%"}
[Stefan]{.absolute top="9.5em" left="4.5em" style="font-size: 0.9em; "}
[@pantaray]{.absolute top="10.5em" left="3.5em" style="font-size: 0.9em; "}

![](imgs/katharine.jpg){.absolute top="4em" left="10.5em" width="16%"}
[Katharine]{.absolute top="9.5em" left="12em" style="font-size: 0.9em; "}
[@KatharineShapcott]{.absolute top="10.5em" left="10em" style="font-size: 0.9em; "}

![](imgs/joscha.jpeg){.absolute top="4em" left="18.7em" width="16%"}
[Joscha]{.absolute top="9.5em" left="21.7em" style="font-size: 0.9em; "}
[@joschaschmiedt]{.absolute top="10.5em" left="20em" style="font-size: 0.9em; "}

![](imgs/acme_logo.png){.absolute top="11em" left="2em" width="15%"}
[[https://github.com/esi-neuroscience/acme](https://github.com/esi-neuroscience/acme)]{.absolute top="14em" left="8em" style="font-size: 0.9em; "}

[**Contact**: [stefan.fuertinger@esi-frankfurt.de](stefan.fuertinger@esi-frankfurt.de)]{.absolute top="16.5em" left="5em" style="font-size: 0.9em; "}

## {background-image="imgs/acme_nilearn_demo.gif" background-opacity="0.2" visibilty="uncounted"}

::: {style="height: 0.8em; "}
:::

::: {.r-fit-text}
Thank You For Your Attention!
:::

[**GitHub**]{.absolute top="10.5em" left="0em" style="font-size: 0.9em; "}

![](imgs/me.jpeg){.absolute top="4em" left="3em" width="16%"}
[Stefan]{.absolute top="9.5em" left="4.5em" style="font-size: 0.9em; "}
[@pantaray]{.absolute top="10.5em" left="3.5em" style="font-size: 0.9em; "}

![](imgs/katharine.jpg){.absolute top="4em" left="10.5em" width="16%"}
[Katharine]{.absolute top="9.5em" left="12em" style="font-size: 0.9em; "}
[@KatharineShapcott]{.absolute top="10.5em" left="10em" style="font-size: 0.9em; "}

![](imgs/joscha.jpeg){.absolute top="4em" left="18.7em" width="16%"}
[Joscha]{.absolute top="9.5em" left="21.7em" style="font-size: 0.9em; "}
[@joschaschmiedt]{.absolute top="10.5em" left="20em" style="font-size: 0.9em; "}

![](imgs/acme_logo.png){.absolute top="11em" left="2em" width="15%"}
[[https://github.com/esi-neuroscience/acme](https://github.com/esi-neuroscience/acme)]{.absolute top="14em" left="8em" style="font-size: 0.9em; "}

[**Contact**: [stefan.fuertinger@esi-frankfurt.de](stefan.fuertinger@esi-frankfurt.de)]{.absolute top="16.5em" left="5em" style="font-size: 0.9em; "}

## Clippings {visibility="hidden"}

- explain fMRI!!!
- fastMRI: 1.5/3.0 Tesla
- many people do stuff embarassingly parallel!
- specify ACME Audience!
- parallelization for people who do not care about parallelization!
- it's GNU parallel w/SLURM
